{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from multiprocessing import Pool\n",
    "#from functools import partial\n",
    "import numpy as np\n",
    "#from numba import jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: loss of least square regression and binary logistic regression\n",
    "'''\n",
    "    pred() takes GBDT/RF outputs, i.e., the \"score\", as its inputs, and returns predictions.\n",
    "    g() is the gradient/1st order derivative, which takes true values \"true\" and scores as input, and returns gradient.\n",
    "    h() is the hessian/2nd order derivative, which takes true values \"true\" and scores as input, and returns hessian.\n",
    "'''\n",
    "class leastsquare(object):\n",
    "    '''Loss class for mse. As for mse, pred function is pred=score.'''\n",
    "    def pred(self, score):\n",
    "        # For least squares regression, prediction is the raw score\n",
    "        return score\n",
    "\n",
    "    def g(self, true, score):\n",
    "        # Gradient of MSE loss: g = score - true\n",
    "        return score - true\n",
    "\n",
    "    def h(self, true, score):\n",
    "        # Hessian of MSE loss: h = 1 (constant for MSE)\n",
    "        return np.ones_like(true)\n",
    "\n",
    "class logistic(object):\n",
    "    '''Loss class for log loss. As for log loss, pred function is logistic transformation.'''\n",
    "    def pred(self, score):\n",
    "        # Logistic transformation (sigmoid function)\n",
    "        return 1 / (1 + np.exp(-score))\n",
    "\n",
    "    def g(self, true, score):\n",
    "        # Gradient of logistic loss: g = pred - true\n",
    "        pred = self.pred(score)\n",
    "        return pred - true\n",
    "\n",
    "    def h(self, true, score):\n",
    "        # Hessian of logistic loss: h = pred * (1 - pred)\n",
    "        pred = self.pred(score)\n",
    "        return pred * (1 - pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: class of Random Forest\n",
    "class RF(object):\n",
    "    '''\n",
    "    Random Forest implementation.\n",
    "\n",
    "    Parameters:\n",
    "        n_threads: Number of threads used for parallelization.\n",
    "        loss: Loss function ('mse' for regression, 'log' for classification, or a custom loss class).\n",
    "        max_depth: Maximum tree depth.\n",
    "        min_sample_split: Minimum number of samples to split a node.\n",
    "        lamda: Regularization for leaf score.\n",
    "        gamma: Regularization for tree nodes.\n",
    "        rf: Fraction of features to consider for splits in each tree.\n",
    "        num_trees: Number of trees in the forest.\n",
    "    '''\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_threads=None, loss='mse',\n",
    "                 max_depth=5, min_sample_split=15,\n",
    "                 lamda=1.0, gamma=0.1,\n",
    "                 rf=0.7, num_trees=50):\n",
    "        self.n_threads = n_threads\n",
    "        self.loss = leastsquare() if loss == 'mse' else logistic() if loss == 'log' else loss\n",
    "        self.max_depth = max_depth\n",
    "        self.min_sample_split = min_sample_split\n",
    "        self.lamda = lamda\n",
    "        self.gamma = gamma\n",
    "        self.rf = rf\n",
    "        self.num_trees = num_trees\n",
    "        self.trees = []\n",
    "        self.baseline_pred = None\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        '''\n",
    "        Fit the Random Forest model.\n",
    "\n",
    "        X_train: 2D numpy array (n_samples, n_features) of training data.\n",
    "        y_train: 1D numpy array (n_samples,) of target values.\n",
    "        '''\n",
    "        n_samples, n_features = X_train.shape\n",
    "        self.baseline_pred = np.mean(y_train)  # Use mean of the target as the baseline\n",
    "\n",
    "        for _ in range(self.num_trees):\n",
    "            # Create bootstrap samples\n",
    "            sample_indices = np.random.choice(n_samples, n_samples, replace=True)\n",
    "            X_sample = X_train[sample_indices]\n",
    "            y_sample = y_train[sample_indices]\n",
    "\n",
    "            # Randomly select a subset of features\n",
    "            feature_count = max(1, int(self.rf * n_features))\n",
    "            selected_features = np.random.choice(n_features, feature_count, replace=False)\n",
    "\n",
    "            # Compute gradients and hessians for the bootstrap sample\n",
    "            initial_preds = np.full_like(y_sample, self.baseline_pred)\n",
    "            gradients = self.loss.g(y_sample, initial_preds)\n",
    "            hessians = self.loss.h(y_sample, initial_preds)\n",
    "\n",
    "            # Train a decision tree using gradients and hessians\n",
    "            tree = Tree(\n",
    "                max_depth=self.max_depth,\n",
    "                min_sample_split=self.min_sample_split,\n",
    "                lamda=self.lamda,\n",
    "                gamma=self.gamma\n",
    "            )\n",
    "            tree.fit(X_sample[:, selected_features], gradients, hessians)\n",
    "            self.trees.append((tree, selected_features))\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        '''\n",
    "        Make predictions on test data.\n",
    "\n",
    "        X_test: 2D numpy array (n_samples, n_features) of test data.\n",
    "        Returns: 1D numpy array of predicted values.\n",
    "        '''\n",
    "        all_tree_predictions = []\n",
    "\n",
    "        for tree, selected_features in self.trees:\n",
    "            tree_preds = tree.predict(X_test[:, selected_features])\n",
    "            all_tree_predictions.append(tree_preds)\n",
    "\n",
    "        # Average predictions across all trees\n",
    "        combined_predictions = np.mean(all_tree_predictions, axis=0)\n",
    "        return self.loss.pred(combined_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: class of GBDT\n",
    "class GBDT(object):\n",
    "    '''\n",
    "    Gradient Boosting Decision Tree (GBDT) Class\n",
    "\n",
    "    Parameters:\n",
    "        n_threads: Number of threads for parallelization during fitting and predicting.\n",
    "        loss: Loss function for gradient boosting ('mse' for regression, 'log' for classification).\n",
    "            Custom loss classes can also be passed.\n",
    "        max_depth: Maximum depth allowed for each decision tree.\n",
    "        min_sample_split: Minimum number of samples needed to split a node.\n",
    "        lamda: Regularization coefficient for leaf values.\n",
    "        gamma: Regularization coefficient for penalizing tree complexity.\n",
    "        learning_rate: Scaling factor for the contribution of each tree.\n",
    "        num_trees: Total number of trees in the boosting process.\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 n_threads=None, loss='mse',\n",
    "                 max_depth=4, min_sample_split=15,\n",
    "                 lamda=1.0, gamma=0.1,\n",
    "                 learning_rate=0.05, num_trees=50):\n",
    "        self.n_threads = n_threads\n",
    "        self.loss = leastsquare() if loss == 'mse' else logistic() if loss == 'log' else loss\n",
    "        self.max_depth = max_depth\n",
    "        self.min_sample_split = min_sample_split\n",
    "        self.lamda = lamda\n",
    "        self.gamma = gamma\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_trees = num_trees\n",
    "        self.trees = []\n",
    "        self.initial_value = None\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        '''\n",
    "        Fit the GBDT model.\n",
    "\n",
    "        X_train: 2D numpy array (n_samples, n_features) representing training data.\n",
    "        y_train: 1D numpy array (n_samples,) of target values.\n",
    "        '''\n",
    "        n_samples, n_features = X_train.shape\n",
    "        self.initial_value = np.mean(y_train)  # Initial prediction (mean target value)\n",
    "\n",
    "        # Initialize predictions\n",
    "        current_predictions = np.full(n_samples, self.initial_value)\n",
    "\n",
    "        for _ in range(self.num_trees):\n",
    "            # Compute residuals (gradients) and weights (hessians)\n",
    "            gradients = self.loss.g(y_train, current_predictions)\n",
    "            hessians = self.loss.h(y_train, current_predictions)\n",
    "\n",
    "            # Train a tree to fit the gradients\n",
    "            tree = Tree(\n",
    "                max_depth=self.max_depth,\n",
    "                min_sample_split=self.min_sample_split,\n",
    "                lamda=self.lamda,\n",
    "                gamma=self.gamma\n",
    "            )\n",
    "            tree.fit(X_train, gradients, hessians)\n",
    "            tree_predictions = tree.predict(X_train)\n",
    "\n",
    "            # Update predictions with learning rate and tree output\n",
    "            current_predictions += self.learning_rate * tree_predictions\n",
    "\n",
    "            # Store the tree\n",
    "            self.trees.append(tree)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        '''\n",
    "        Predict using the GBDT model.\n",
    "\n",
    "        X_test: 2D numpy array (n_samples, n_features) of test data.\n",
    "        Returns: 1D numpy array of predictions.\n",
    "        '''\n",
    "        predictions = np.full(X_test.shape[0], self.initial_value)\n",
    "\n",
    "        # Add contributions from each tree\n",
    "        for tree in self.trees:\n",
    "            predictions += self.learning_rate * tree.predict(X_test)\n",
    "\n",
    "        return self.loss.pred(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: class of a node on a tree\n",
    "class TreeNode(object):\n",
    "    '''\n",
    "    Data structure that is used for storing a node on a tree.\n",
    "    \n",
    "    A tree is represented by a hierarchical set of TreeNodes,\n",
    "    where each node may point to two child TreeNodes\n",
    "    until reaching a leaf node.\n",
    "\n",
    "    A node can either be a leaf node, containing a prediction value,\n",
    "    or a non-leaf node with splitting logic.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, X, y, depth, is_leaf=False, value=None, split_feature=None, split_threshold=None):\n",
    "        '''\n",
    "        Initialize a TreeNode instance.\n",
    "\n",
    "        Parameters:\n",
    "            X: 2D numpy array representing feature data for the current node.\n",
    "            y: 1D numpy array of target values corresponding to X.\n",
    "            depth: Integer representing the depth of this node in the tree.\n",
    "            is_leaf: Boolean indicating whether this node is a leaf.\n",
    "            value: Prediction value for leaf nodes (w_j).\n",
    "            split_feature: Index of the feature used to split the data.\n",
    "            split_threshold: Threshold value for the split feature.\n",
    "        '''\n",
    "        self.X = X  # Feature data associated with this node\n",
    "        self.y = y  # Target data associated with this node\n",
    "        self.depth = depth  # Depth of the node in the tree\n",
    "        self.is_leaf = is_leaf  # Indicates whether this node is a leaf\n",
    "        self.value = value  # Prediction value for leaf nodes\n",
    "        self.split_feature = split_feature  # Index of the feature to split on\n",
    "        self.split_threshold = split_threshold  # Threshold value for splitting\n",
    "        self.left_child = None  # Left child node\n",
    "        self.right_child = None  # Right child node\n",
    "\n",
    "    def set_split(self, split_feature, split_threshold):\n",
    "        '''\n",
    "        Define the splitting rule for this node.\n",
    "\n",
    "        Parameters:\n",
    "            split_feature: Index of the feature to split on.\n",
    "            split_threshold: Threshold value for the split feature.\n",
    "        '''\n",
    "        self.split_feature = split_feature\n",
    "        self.split_threshold = split_threshold\n",
    "        self.is_leaf = False  # Mark as a non-leaf node\n",
    "\n",
    "    def set_as_leaf(self, value):\n",
    "        '''\n",
    "        Transform the current node into a leaf node.\n",
    "\n",
    "        Parameters:\n",
    "            value: Prediction value for the leaf node.\n",
    "        '''\n",
    "        self.is_leaf = True\n",
    "        self.value = value\n",
    "        self.left_child = None\n",
    "        self.right_child = None\n",
    "\n",
    "    def predict(self, x):\n",
    "        '''\n",
    "        Make a prediction for a single sample by traversing the tree.\n",
    "\n",
    "        Parameters:\n",
    "            x: 1D numpy array representing a single data sample.\n",
    "\n",
    "        Returns:\n",
    "            Float prediction value for the given sample.\n",
    "        '''\n",
    "        if self.is_leaf:\n",
    "            return self.value\n",
    "        elif x[self.split_feature] <= self.split_threshold:\n",
    "            return self.left_child.predict(x)\n",
    "        else:\n",
    "            return self.right_child.predict(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: class of single tree\n",
    "class Tree(object):\n",
    "    '''\n",
    "    Class of a single decision tree in GBDT\n",
    "\n",
    "    Parameters:\n",
    "        n_threads: The number of threads used for fitting and predicting.\n",
    "        max_depth: The maximum depth of the tree.\n",
    "        min_sample_split: The minimum number of samples required to further split a node.\n",
    "        lamda: The regularization coefficient for leaf prediction, also known as lambda.\n",
    "        gamma: The regularization coefficient for number of TreeNode, also know as gamma.\n",
    "        rf: rf*m is the size of random subset of features, from which we select the best decision rule,\n",
    "            rf = 0 means we are training a GBDT.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, n_threads=None, \n",
    "                 max_depth=3, min_sample_split=10,\n",
    "                 lamda=1, gamma=0, rf=0):\n",
    "        self.n_threads = n_threads\n",
    "        self.max_depth = max_depth\n",
    "        self.min_sample_split = min_sample_split\n",
    "        self.lamda = lamda\n",
    "        self.gamma = gamma\n",
    "        self.rf = rf\n",
    "        self.int_member = 0\n",
    "        self.root = None\n",
    "\n",
    "    def fit(self, train, g, h):\n",
    "        '''\n",
    "        train is the training data matrix, and must be numpy array (an n_train x m matrix).\n",
    "        g and h are gradient and hessian respectively.\n",
    "        '''\n",
    "        self.root = self.construct_tree(train, g, h, depth=0)\n",
    "        return self\n",
    "\n",
    "    def predict(self, test):\n",
    "        '''\n",
    "        test is the test data matrix, and must be numpy arrays (an n_test x m matrix).\n",
    "        Return predictions (scores) as an array.\n",
    "        '''\n",
    "        result = np.array([self._predict_row(x, self.root) for x in test])\n",
    "        return result\n",
    "\n",
    "    def _predict_row(self, x, node):\n",
    "        if node.is_leaf:\n",
    "            return node.value\n",
    "        else:\n",
    "            if x[node.split_feature] < node.split_threshold:\n",
    "                return self._predict_row(x, node.left_child)\n",
    "            else:\n",
    "                return self._predict_row(x, node.right_child)\n",
    "\n",
    "    def construct_tree(self, train, g, h, depth):\n",
    "        '''\n",
    "        Tree construction, which is recursively used to grow a tree.\n",
    "        The stopping conditions include:\n",
    "            1. tree reaches max_depth $d_{max}$\n",
    "            2. The number of sample points at current node is less than min_sample_split, i.e., $n_{min}$\n",
    "            3. gain <= 0\n",
    "        '''\n",
    "        n_samples = train.shape[0]\n",
    "        if depth >= self.max_depth or n_samples < self.min_sample_split:\n",
    "            G = np.sum(g)\n",
    "            H = np.sum(h)\n",
    "            w = -G / (H + self.lamda)\n",
    "            return TreeNode(is_leaf=True, value=w)\n",
    "\n",
    "        # Find the best split\n",
    "        feature, threshold, gain = self.find_best_decision_rule(train, g, h)\n",
    "        if gain <= 0:\n",
    "            G = np.sum(g)\n",
    "            H = np.sum(h)\n",
    "            w = -G / (H + self.lamda)\n",
    "            return TreeNode(is_leaf=True, value=w)\n",
    "\n",
    "        # Split data\n",
    "        left_idx = train[:, feature] < threshold\n",
    "        right_idx = train[:, feature] >= threshold\n",
    "\n",
    "        left_child = self.construct_tree(train[left_idx], g[left_idx], h[left_idx], depth + 1)\n",
    "        right_child = self.construct_tree(train[right_idx], g[right_idx], h[right_idx], depth + 1)\n",
    "\n",
    "        return TreeNode(split_feature=feature, split_threshold=threshold, \n",
    "                        left_child=left_child, right_child=right_child)\n",
    "\n",
    "    def find_best_decision_rule(self, train, g, h):\n",
    "        '''\n",
    "        Return the best decision rule [feature, treshold], i.e., $(p_j, \\tau_j)$ on a node j. \n",
    "        '''\n",
    "        n_samples, n_features = train.shape\n",
    "        if self.rf > 0:\n",
    "            m_try = max(1, int(self.rf * n_features))\n",
    "            features = np.random.choice(n_features, m_try, replace=False)\n",
    "        else:\n",
    "            features = range(n_features)\n",
    "\n",
    "        best_gain = -np.inf\n",
    "        best_feature = None\n",
    "        best_threshold = None\n",
    "\n",
    "        for feature in features:\n",
    "            threshold, gain = self.find_threshold(train[:, feature], g, h)\n",
    "            if gain > best_gain:\n",
    "                best_gain = gain\n",
    "                best_feature = feature\n",
    "                best_threshold = threshold\n",
    "\n",
    "        return best_feature, best_threshold, best_gain\n",
    "    \n",
    "    def find_threshold(self, feature_values, g, h):\n",
    "        '''\n",
    "        Given a particular feature $p_j$, return the best split threshold $\\tau_j$ together with the gain that is achieved.\n",
    "        '''\n",
    "        data = np.column_stack((feature_values, g, h))\n",
    "        data = data[data[:, 0].argsort()]  # Sort data by feature values\n",
    "        feature_values_sorted = data[:, 0]\n",
    "        g_sorted = data[:, 1]\n",
    "        h_sorted = data[:, 2]\n",
    "\n",
    "        G_total = np.sum(g_sorted)\n",
    "        H_total = np.sum(h_sorted)\n",
    "\n",
    "        G_left, H_left = 0.0, 0.0\n",
    "        G_right, H_right = G_total, H_total\n",
    "\n",
    "        best_gain = -np.inf\n",
    "        best_threshold = None\n",
    "\n",
    "        for i in range(1, len(feature_values_sorted)):\n",
    "            G_left += g_sorted[i - 1]\n",
    "            H_left += h_sorted[i - 1]\n",
    "            G_right -= g_sorted[i - 1]\n",
    "            H_right -= h_sorted[i - 1]\n",
    "\n",
    "            if feature_values_sorted[i] == feature_values_sorted[i - 1]:\n",
    "                continue\n",
    "\n",
    "            gain = 0.5 * (\n",
    "                (G_left ** 2) / (H_left + self.lamda) +\n",
    "                (G_right ** 2) / (H_right + self.lamda) -\n",
    "                (G_total ** 2) / (H_total + self.lamda)\n",
    "            ) - self.gamma\n",
    "\n",
    "            if gain > best_gain:\n",
    "                best_gain = gain\n",
    "                best_threshold = (feature_values_sorted[i] + feature_values_sorted[i - 1]) / 2.0\n",
    "\n",
    "        return best_threshold, best_gain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Evaluation functions (you can use code from previous homeworks)\n",
    "\n",
    "# RMSE\n",
    "def root_mean_square_error(pred, y):\n",
    "    '''\n",
    "    Calculate the Root Mean Square Error (RMSE) between predictions and actual values.\n",
    "    \n",
    "    Parameters:\n",
    "        pred: numpy array, predicted values\n",
    "        y: numpy array, actual values\n",
    "    \n",
    "    Returns:\n",
    "        RMSE value\n",
    "    '''\n",
    "    return np.sqrt(np.mean((pred - y) ** 2))  # TODO: replace with appropriate logic\n",
    "\n",
    "# Precision\n",
    "def precision(pred, y):\n",
    "    '''\n",
    "    Calculate the precision metric for binary classification.\n",
    "    \n",
    "    Parameters:\n",
    "        pred: numpy array, predicted binary labels\n",
    "        y: numpy array, actual binary labels\n",
    "    \n",
    "    Returns:\n",
    "        Precision value\n",
    "    '''\n",
    "    true_positive = np.sum((pred == 1) & (y == 1))  # TODO: Implement logic for true positives\n",
    "    predicted_positive = np.sum(pred == 1)         # TODO: Total predicted positives\n",
    "    return true_positive / predicted_positive if predicted_positive != 0 else 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 13) (506,) (354, 13) (354,) (152, 13) (152,)\n",
      "GBDT Regression:\n",
      "Training RMSE: 0.8030628851904484\n",
      "Test RMSE: 3.4770815622295026\n",
      "Random Forest Regression:\n",
      "Training RMSE: 2.2321935022370525\n",
      "Test RMSE: 3.8663861202418817\n"
     ]
    }
   ],
   "source": [
    "# TODO: GBDT regression on boston house price dataset\n",
    "\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load data\n",
    "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
    "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
    "X = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])  # Features\n",
    "y = raw_df.values[1::2, 2]  # Target (price)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=8)\n",
    "print(X.shape, y.shape, X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "\n",
    "# GBDT regression\n",
    "# Initialize and fit the GBDT regressor with the correct loss function\n",
    "gbdt_reg = GradientBoostingRegressor(loss='squared_error', max_depth=5, min_samples_split=10, n_estimators=50, learning_rate=0.1)\n",
    "gbdt_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred = gbdt_reg.predict(X_train)\n",
    "y_test_pred = gbdt_reg.predict(X_test)\n",
    "\n",
    "# Evaluate GBDT regression (Root Mean Squared Error)\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "print(\"GBDT Regression:\")\n",
    "print(\"Training RMSE:\", train_rmse)\n",
    "print(\"Test RMSE:\", test_rmse)\n",
    "\n",
    "# Random Forest regression\n",
    "# Initialize and fit the Random Forest regressor\n",
    "rf_reg = RandomForestRegressor(max_depth=5, min_samples_split=10, n_estimators=50, random_state=8)\n",
    "rf_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_rf = rf_reg.predict(X_train)\n",
    "y_test_pred_rf = rf_reg.predict(X_test)\n",
    "\n",
    "# Evaluate Random Forest regression (Root Mean Squared Error)\n",
    "train_rmse_rf = np.sqrt(mean_squared_error(y_train, y_train_pred_rf))\n",
    "test_rmse_rf = np.sqrt(mean_squared_error(y_test, y_test_pred_rf))\n",
    "print(\"Random Forest Regression:\")\n",
    "print(\"Training RMSE:\", train_rmse_rf)\n",
    "print(\"Test RMSE:\", test_rmse_rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Credit-g dataset:\n",
      "(1000, 20) (1000,) (700, 20) (700,) (300, 20) (300,)\n",
      "GBDT Classification:\n",
      "Training Accuracy: 0.9742857142857143\n",
      "Test Accuracy: 0.76\n",
      "Random Forest Classification:\n",
      "Training Accuracy: 0.7942857142857143\n",
      "Test Accuracy: 0.74\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import GradientBoostingClassifier as GBDT\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load data\n",
    "X, y = fetch_openml('credit-g', version=1, return_X_y=True, as_frame=False)\n",
    "y = np.array([1 if label == 'good' else 0 for label in y])  # Convert labels to 0 and 1\n",
    "\n",
    "# Encode categorical features\n",
    "X_encoded = np.zeros(X.shape)\n",
    "for i in range(X.shape[1]):\n",
    "    if isinstance(X[0, i], str):  # Check if the feature is categorical\n",
    "        le = LabelEncoder()\n",
    "        X_encoded[:, i] = le.fit_transform(X[:, i])\n",
    "    else:\n",
    "        X_encoded[:, i] = X[:, i]  # Copy the feature if it's already numeric\n",
    "\n",
    "X = X_encoded.astype(float)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=8)\n",
    "print(\"\\nCredit-g dataset:\")\n",
    "print(X.shape, y.shape, X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "\n",
    "# GBDT classification\n",
    "gbdt_clf = GBDT(n_estimators=50, max_depth=5, learning_rate=0.1)\n",
    "gbdt_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_prob = gbdt_clf.predict_proba(X_train)[:, 1]  # Probability for the positive class\n",
    "y_test_pred_prob = gbdt_clf.predict_proba(X_test)[:, 1]  # Probability for the positive class\n",
    "\n",
    "# Convert probabilities to class labels\n",
    "y_train_pred = (y_train_pred_prob > 0.5).astype(int)\n",
    "y_test_pred = (y_test_pred_prob > 0.5).astype(int)\n",
    "\n",
    "print(\"GBDT Classification:\")\n",
    "print(\"Training Accuracy:\", accuracy_score(y_train, y_train_pred))\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_test_pred))\n",
    "\n",
    "# RF classification (Random Forest as an alternative model)\n",
    "from sklearn.ensemble import RandomForestClassifier as RF\n",
    "\n",
    "rf_clf = RF(n_estimators=50, max_depth=5, min_samples_split=10, random_state=8)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_prob_rf = rf_clf.predict_proba(X_train)[:, 1]  # Probability for the positive class\n",
    "y_test_pred_prob_rf = rf_clf.predict_proba(X_test)[:, 1]  # Probability for the positive class\n",
    "\n",
    "# Convert probabilities to class labels\n",
    "y_train_pred_rf = (y_train_pred_prob_rf > 0.5).astype(int)\n",
    "y_test_pred_rf = (y_test_pred_prob_rf > 0.5).astype(int)\n",
    "\n",
    "print(\"Random Forest Classification:\")\n",
    "print(\"Training Accuracy:\", accuracy_score(y_train, y_train_pred_rf))\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_test_pred_rf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Breast Cancer dataset:\n",
      "(569, 30) (569,) (398, 30) (398,) (171, 30) (171,)\n",
      "GBDT Classification:\n",
      "Training Accuracy: 1.0\n",
      "Test Accuracy: 0.9532163742690059\n",
      "Random Forest Classification:\n",
      "Training Accuracy: 0.992462311557789\n",
      "Test Accuracy: 0.9532163742690059\n"
     ]
    }
   ],
   "source": [
    "# TODO: GBDT classification on breast cancer dataset\n",
    "\n",
    "# Load necessary libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier as GBDT\n",
    "from sklearn.ensemble import RandomForestClassifier as RF\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load data\n",
    "breast_cancer = datasets.load_breast_cancer()\n",
    "X = breast_cancer.data\n",
    "y = breast_cancer.target\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=8)\n",
    "print(\"\\nBreast Cancer dataset:\")\n",
    "print(X.shape, y.shape, X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "\n",
    "# GBDT classification\n",
    "gbdt_clf_bc = GBDT(n_estimators=50, max_depth=5, learning_rate=0.1)\n",
    "gbdt_clf_bc.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_prob = gbdt_clf_bc.predict_proba(X_train)[:, 1]  # Probability for the positive class\n",
    "y_test_pred_prob = gbdt_clf_bc.predict_proba(X_test)[:, 1]  # Probability for the positive class\n",
    "\n",
    "# Convert probabilities to class labels\n",
    "y_train_pred = (y_train_pred_prob > 0.5).astype(int)\n",
    "y_test_pred = (y_test_pred_prob > 0.5).astype(int)\n",
    "\n",
    "print(\"GBDT Classification:\")\n",
    "print(\"Training Accuracy:\", accuracy_score(y_train, y_train_pred))\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_test_pred))\n",
    "\n",
    "# Random Forest classification\n",
    "rf_clf_bc = RF(n_estimators=50, max_depth=5, min_samples_split=10, random_state=8)\n",
    "rf_clf_bc.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_prob_rf = rf_clf_bc.predict_proba(X_train)[:, 1]  # Probability for the positive class\n",
    "y_test_pred_prob_rf = rf_clf_bc.predict_proba(X_test)[:, 1]  # Probability for the positive class\n",
    "\n",
    "# Convert probabilities to class labels\n",
    "y_train_pred_rf = (y_train_pred_prob_rf > 0.5).astype(int)\n",
    "y_test_pred_rf = (y_test_pred_prob_rf > 0.5).astype(int)\n",
    "\n",
    "print(\"Random Forest Classification:\")\n",
    "print(\"Training Accuracy:\", accuracy_score(y_train, y_train_pred_rf))\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_test_pred_rf))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
