{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"18eDzQGt5SSrvrllfNG7SKcOu8JdGJT-f","timestamp":1738241047901}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4zpmoVoBdkVM","executionInfo":{"status":"ok","timestamp":1734504269446,"user_tz":-420,"elapsed":16566,"user":{"displayName":"Syed Ali Haider","userId":"08852981343537336738"}},"outputId":"5ffed8f5-dbae-4a41-c0c3-d02229166479"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/480.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/116.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/179.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/134.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/194.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["!pip install datasets -q\n","from torch.utils.data import DataLoader, Dataset, random_split\n","import random\n","import numpy as np\n","import torch\n","import torchvision.transforms as transforms\n","from torch.utils.data import Dataset, DataLoader\n","from datasets import load_dataset\n","from sklearn.metrics import confusion_matrix\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import cv2\n","import numpy as np\n","from collections import defaultdict\n","from torch.utils.data import Dataset, DataLoader, random_split\n","from torchvision import transforms\n","from datasets import load_dataset\n","from torchvision.models import resnet18, resnet50,resnet152, densenet121, densenet201\n","import os\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch\n","from torch.optim import Adam\n","import torch\n","from tqdm import tqdm\n","from sklearn.metrics import f1_score\n","import pandas as pd\n","\n","# Set seed for reproducibility\n","SEED = 42\n","random.seed(SEED)\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","if torch.cuda.is_available():\n","    torch.cuda.manual_seed(SEED)\n","    torch.cuda.manual_seed_all(SEED)\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","source":["\n","# Function to check if an image is blurry\n","def is_blurry(image, threshold=90.0):\n","    # Ensure the image is a NumPy array\n","    if not isinstance(image, np.ndarray):\n","        image = np.array(image)\n","    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n","    laplacian_var = cv2.Laplacian(gray_image, cv2.CV_64F).var()\n","    return laplacian_var < threshold\n","\n","\n","# Function to compute image hash\n","def dhash(image, hash_size=8):\n","    # Ensure the image is a NumPy array\n","    if not isinstance(image, np.ndarray):\n","        image = np.array(image)\n","    # Convert to grayscale if necessary\n","    if len(image.shape) == 3:  # Check if the image has color channels\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n","    resized = cv2.resize(image, (hash_size + 1, hash_size))\n","    diff = resized[:, 1:] > resized[:, :-1]\n","    return sum([2 ** i for (i, v) in enumerate(diff.flatten()) if v])\n","\n","\n","# Load dataset\n","dataset = load_dataset('hmdliu/ACAC-4K')\n","\n","# Filter out blurry images\n","non_blurry_images = [example for example in dataset['train'] if not is_blurry(example['image'])]\n","\n","\n","# Remove duplicates\n","hashes = defaultdict(list)\n","for idx, example in enumerate(non_blurry_images):\n","    image_hash = dhash(example['image'])\n","    hashes[image_hash].append(example)\n","\n","# Remove duplicates by keeping only one example per hash\n","unique_images = [examples[0] for examples in hashes.values()]\n","\n","\n","# Custom Dataset Class\n","class CustomHFDataset(Dataset):\n","    def __init__(self, hf_dataset, transform=None, test_flag=False):\n","        self.hf_dataset = hf_dataset\n","        self.transform = transform\n","        self.test_flag = test_flag\n","\n","    def __len__(self):\n","        return len(self.hf_dataset)\n","\n","    def __getitem__(self, idx):\n","        example = self.hf_dataset[idx]\n","        image = example['image']\n","        label = example['label']\n","        if self.transform:\n","            image = self.transform(image)\n","        if self.test_flag:\n","            return image, example['idx']\n","        else:\n","            return image, label\n","\n","# Define transformations with data augmentation\n","transform_train = transforms.Compose([\n","    transforms.Resize(512),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.RandomRotation(15),\n","    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.6127, 0.5398, 0.4442], std=[0.1288, 0.1147, 0.1009])\n","])\n","\n","transform_test = transforms.Compose([\n","    transforms.Resize(512),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.6127, 0.5398, 0.4442], std=[0.1288, 0.1147, 0.1009])\n","])\n","\n","train_data = CustomHFDataset(unique_images, transform=transform_train)\n","test_data = CustomHFDataset(dataset['test'], transform=transform_test)\n","\n","# Split the training data\n","num_samples = len(train_data)\n","test_split_size = 200\n","train_split_size = num_samples - test_split_size\n","\n","train_dataset, extra_test_dataset = random_split(\n","    train_data,\n","    [train_split_size, test_split_size],\n","    generator=torch.Generator().manual_seed(SEED)\n",")\n","\n","# Create DataLoaders\n","train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n","test_loader = DataLoader(extra_test_dataset, batch_size=16, shuffle=False)\n","kaggle_test_loader = DataLoader(test_data, batch_size=16, shuffle=False)\n","\n","print(f\"Train set size: {len(train_dataset)}\")\n","print(f\"Extra test set size: {len(extra_test_dataset)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EPfVyWK7ehsK","executionInfo":{"status":"ok","timestamp":1734506465032,"user_tz":-420,"elapsed":570,"user":{"displayName":"Syed Ali Haider","userId":"08852981343537336738"}},"outputId":"d6b43f07-e174-4b8e-ed05-292420bab978"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Train set size: 2885\n","Extra test set size: 200\n"]}]},{"cell_type":"code","source":["# from torchvision.models import vit_b_16\n","\n","# Initialize models without pretrained weights\n","ResNet18_cleaner = resnet18(weights=None)\n","models = {\n","    \"ResNet18\": resnet18(weights=None),\n","    \"ResNet50\": resnet50(weights=None),\n","    \"ResNet152\": resnet152(weights=None),\n","    \"DenseNet121\": densenet121(weights=None),\n","    \"DenseNet201\": densenet201(weights=None),\n","    # \"ViT\": vit_b_16(weights=None)\n","}\n","\n","# Modify the final layer to match the number of classes (e.g., 10 classes)\n","num_classes = len(dataset['train'].features['label'].names)\n","\n","\n","# Iterate through the models and adjust the final layer\n","for name, model in models.items():\n","    if 'ResNet' in name or 'DenseNet' in name:\n","        if hasattr(model, 'fc'):  # ResNet\n","            model.fc = torch.nn.Linear(model.fc.in_features, num_classes)\n","        elif hasattr(model, 'classifier'):  # DenseNet\n","            model.classifier = torch.nn.Linear(model.classifier.in_features, num_classes)\n","    elif 'ViT' in name:  # For ViT (Vision Transformer)\n","        # Check for the attribute to modify\n","        if hasattr(model, 'head'):  # ViT model with head\n","            model.head = torch.nn.Linear(model.head.in_features, num_classes)\n","        elif hasattr(model, 'classifier'):  # ViT model with classifier\n","            model.classifier = torch.nn.Linear(model.classifier.in_features, num_classes)\n","\n","    # Print the updated model architecture\n","    print(f\"Updated {name} model:\")\n","\n","models = {name: model.to(device) for name, model in models.items()}\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4BCtdHY1enaR","executionInfo":{"status":"ok","timestamp":1734505182943,"user_tz":-420,"elapsed":3175,"user":{"displayName":"Syed Ali Haider","userId":"08852981343537336738"}},"outputId":"83602f7e-c1f2-4357-d9ae-92903864ea10"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Updated ResNet18 model:\n","Updated ResNet50 model:\n","Updated ResNet152 model:\n","Updated DenseNet121 model:\n","Updated DenseNet201 model:\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","drive_model_dir = \"/kaggle/models\""],"metadata":{"id":"ZiFWhta8ep7O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","class FocalLoss(nn.Module):\n","    def __init__(self, gamma=3, alpha=0.2):\n","        super(FocalLoss, self).__init__()\n","        self.gamma = gamma\n","        self.alpha = alpha\n","\n","    def forward(self, inputs, targets):\n","        BCE_loss = F.cross_entropy(inputs, targets, reduction='mean')\n","        pt = torch.exp(-BCE_loss)\n","        focal_loss = ((1 - pt) ** self.gamma * BCE_loss).mean()\n","\n","\n","        return focal_loss.mean()\n","\n","\n","\n","criterion = FocalLoss()\n","\n","\n"],"metadata":{"id":"7vpYq2yterhC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_model(model, optimizer, criterion, train_loader):\n","    model.train()\n","    total_loss = 0\n","\n","    for images, labels in train_loader:\n","        images, labels = images.to(device), labels.to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += loss.item()\n","\n","    return total_loss / len(train_loader)\n","\n","def evaluate_model(model, test_loader):\n","    model.eval()\n","    all_preds = []\n","    all_labels = []\n","\n","    with torch.no_grad():\n","        for images, labels in test_loader:\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = model(images)\n","            preds = outputs.argmax(dim=1).cpu().numpy()\n","            all_preds.extend(preds)\n","            all_labels.extend(labels.cpu().numpy())\n","\n","    return all_preds, all_labels\n","\n","def plot_confusion_matrix(y_true, y_pred):\n","    cm = confusion_matrix(y_true, y_pred)\n","    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n","    plt.xlabel('Predicted')\n","    plt.ylabel('Actual')\n","    plt.show()\n"],"metadata":{"id":"aqKsLSC4euFY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Evaluate Function\n","def test_evaluate_model(model, dataloader):\n","    model.eval()\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for images, labels in dataloader:\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = model(images)\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","    return print(name,\":\",100 * correct / total)"],"metadata":{"id":"t3RSPYsjewAR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_and_save_model(model, model_name, train_loader, num_epochs=12):\n","    optimizer = Adam(model.parameters(), lr=0.0005)\n","    criterion = FocalLoss()\n","\n","    # criterion = nn.CrossEntropyLoss()\n","    # optimizer = torch.optim.Adam(model.parameters(), lr=0.00005)\n","    for epoch in range(num_epochs):\n","        model.train()\n","        total_loss = 0\n","\n","        for images, labels in train_loader:\n","            images, labels = images.to(device), labels.to(device)\n","\n","            optimizer.zero_grad()\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","\n","            loss.backward()\n","            optimizer.step()\n","\n","            total_loss += loss.item()\n","\n","        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(train_loader):.4f}\")\n","\n","    # Save the model state to Google Drive\n","    test_evaluate_model(model, test_loader)\n","    model_path = f'{drive_model_dir}/{model_name}.pth'\n","    torch.save(model.state_dict(), model_path)\n","    print(f\"{model_name} saved to {model_path}\")\n","\n","# Train and save each model one by one\n","# for name in models:\n","name = \"ResNet18_cleaner\"\n","print(f\"Training {name}...\")\n","model = ResNet18_cleaner.to(device)\n","train_and_save_model(model, name, train_loader,4)\n","del model  # Delete the model to free up GPU memory\n","torch.cuda.empty_cache()  # Clear GPU cache\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TEi8tj-FeyI3","executionInfo":{"status":"ok","timestamp":1734506311035,"user_tz":-420,"elapsed":882,"user":{"displayName":"Syed Ali Haider","userId":"08852981343537336738"}},"outputId":"988bb8b6-0106-45b2-85de-64e56a63806b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["ResNet18_cleaner saved to /kaggle/models/ResNet18_cleaner.pth\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"YEQv_ddHok5K"}},{"cell_type":"code","source":["accuracy_extra_test = test_evaluate_model(ResNet18_cleaner, test_loader)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0zHV4o-we13i","executionInfo":{"status":"ok","timestamp":1734506385740,"user_tz":-420,"elapsed":12189,"user":{"displayName":"Syed Ali Haider","userId":"08852981343537336738"}},"outputId":"d02efed1-bf21-40aa-fd70-5445707065ba"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["ResNet18_cleaner : 58.5\n"]}]},{"cell_type":"code","source":["import torch.nn.functional as F\n","from torch.utils.data import Subset\n","\n","def test_and_filter(model, test_loader, train_dataset, threshold=0.4):\n","    \"\"\"\n","    Test the model and identify high-confidence incorrect predictions.\n","    Remove these samples from the training dataset.\n","    \"\"\"\n","    model.eval()\n","    incorrect_high_confidence_indices = []\n","\n","    with torch.no_grad():\n","        for batch_idx, (images, labels) in enumerate(test_loader):\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = model(images)\n","            probs = F.softmax(outputs, dim=1)  # Get probabilities\n","            preds = outputs.argmax(dim=1)  # Get predicted class indices\n","\n","            # Check for high-confidence incorrect predictions\n","            for i in range(len(labels)):\n","                if preds[i] != labels[i] and probs[i][preds[i]] > threshold:\n","                    # Calculate global index in test dataset\n","                    global_idx = batch_idx * test_loader.batch_size + i\n","                    incorrect_high_confidence_indices.append(global_idx)\n","\n","    print(f\"Found {len(incorrect_high_confidence_indices)} high-confidence incorrect predictions.\")\n","\n","    # Filter out these indices from the training dataset\n","    train_dataset = filter_training_data(train_dataset, incorrect_high_confidence_indices)\n","\n","    return train_dataset\n","\n","\n","def filter_training_data(train_dataset, indices_to_remove):\n","    \"\"\"\n","    Remove specified indices from the training dataset.\n","    \"\"\"\n","    # Create a new dataset excluding the specified indices\n","    all_indices = set(range(len(train_dataset)))\n","    filtered_indices = list(all_indices - set(indices_to_remove))\n","\n","    filtered_dataset = Subset(train_dataset, filtered_indices)\n","    print(f\"Training set reduced from {len(train_dataset)} to {len(filtered_dataset)} samples.\")\n","\n","    return filtered_dataset\n","\n","\n","# Example usage\n","# for name in models:\n","name = \"ResNet18_cleaner\"\n","print(f\"Testing and filtering for {name}...\")\n","\n","    # Load model and test it\n","model = ResNet18_cleaner.to(device)\n","train_dataset = test_and_filter(model, test_loader, train_dataset)\n","new_train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FUkjpq8ee3_u","executionInfo":{"status":"ok","timestamp":1734506535024,"user_tz":-420,"elapsed":10914,"user":{"displayName":"Syed Ali Haider","userId":"08852981343537336738"}},"outputId":"5835a150-cd28-4155-9996-644679dbc7b7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Testing and filtering for ResNet18_cleaner...\n","Found 27 high-confidence incorrect predictions.\n","Training set reduced from 2844 to 2817 samples.\n"]}]},{"cell_type":"code","source":["len(train_dataset)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n-Kd2sA0e6Ha","executionInfo":{"status":"ok","timestamp":1734506544476,"user_tz":-420,"elapsed":579,"user":{"displayName":"Syed Ali Haider","userId":"08852981343537336738"}},"outputId":"42f08036-c829-415d-c4f2-d8fc2ade3157"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2817"]},"metadata":{},"execution_count":28}]},{"cell_type":"code","source":["from sklearn.metrics import f1_score\n","import numpy as np\n","\n","def train_and_save_model(model, model_name, train_loader, test_loader, num_epochs=12):\n","    optimizer = torch.optim.Adam(model.parameters(), lr=0.00005)\n","    criterion = FocalLoss(alpha=1.0, gamma=2.0, reduction='mean')  # Use Focal Loss\n","\n","\n","    # Training loop\n","    for epoch in range(num_epochs):\n","        model.train()\n","        total_loss = 0\n","\n","        for images, labels in train_loader:\n","            images, labels = images.to(device), labels.to(device)\n","\n","            optimizer.zero_grad()\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","\n","            loss.backward()\n","            optimizer.step()\n","\n","            total_loss += loss.item()\n","\n","        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(train_loader):.4f}\")\n","\n","    # Evaluate the model on the test set to calculate F1 scores\n","    f1_scores = evaluate_f1_scores(model, test_loader)\n","\n","    # Normalize F1 scores to get weights\n","    weights = f1_scores / np.sum(f1_scores)\n","    print(f\"Class-wise F1 scores for {model_name}: {f1_scores}\")\n","    print(f\"Normalized weights for {model_name}: {weights}\")\n","    test_evaluate_model(model, test_loader)\n","\n","    # Save the model state to Google Drive\n","    model_path = f'{drive_model_dir}/{model_name}.pth'\n","    torch.save(model.state_dict(), model_path)\n","    print(f\"{model_name} saved to {model_path}\")\n","\n","    return weights\n","\n","\n","def evaluate_f1_scores(model, test_loader):\n","    \"\"\"Evaluate class-wise F1 scores on the test set.\"\"\"\n","    model.eval()\n","    all_preds = []\n","    all_labels = []\n","\n","    with torch.no_grad():\n","        for images, labels in test_loader:\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = model(images)\n","            preds = outputs.argmax(dim=1).cpu().numpy()\n","            all_preds.extend(preds)\n","            all_labels.extend(labels.cpu().numpy())\n","\n","    # Compute F1 scores for each class\n","    f1_scores = f1_score(all_labels, all_preds, average=None)  # Class-wise F1 scores\n","    return np.array(f1_scores)\n","\n","\n","# Train and save each model one by one while calculating weights\n","model_weights = {}\n","for name in models:\n","    print(f\"Training {name}...\")\n","    model = models[name].to(device)\n","\n","    # Train and compute weights based on class-wise F1 scores\n","    weights = train_and_save_model(model, name, new_train_loader, test_loader)\n","\n","    # Store weights for the ensemble voting system\n","    model_weights[name] = weights\n","\n","    del model  # Delete the model to free up GPU memory\n","    torch.cuda.empty_cache()  # Clear GPU cache\n","\n","print(\"Model training complete. Weights for ensemble:\")\n","print(model_weights)\n","\n","# The `model_weights` dictionary now contains normalized class-wise weights for each model.\n"],"metadata":{"id":"mrm7Rk4se8kn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Model training complete. Weights for ensemble:\")\n","print(model_weights)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xA3oYMA0_cwS","executionInfo":{"status":"ok","timestamp":1734512337173,"user_tz":-420,"elapsed":774,"user":{"displayName":"Syed Ali Haider","userId":"08852981343537336738"}},"outputId":"2c9e4271-9678-45c1-dcfd-342b3da529a3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model training complete. Weights for ensemble:\n","{'ResNet18': array([0.        , 0.2640688 , 0.09841073, 0.12574705, 0.24231547,\n","       0.26945795]), 'ResNet50': array([0.        , 0.28690981, 0.        , 0.16074228, 0.27783201,\n","       0.2745159 ])}\n"]}]},{"cell_type":"code","source":["def test(model, test_loader, device):\n","    model.eval()\n","    predictions = []\n","    idxs = []\n","\n","    with torch.no_grad():\n","        for images, idx in tqdm(test_loader, desc=\"Testing\"):\n","            images = images.to(device)\n","            outputs = model(images)\n","            _, predicted = outputs.max(1)\n","\n","            predictions.extend(predicted.cpu().numpy())\n","            idxs.extend(idx.numpy())  # Ensure idx is on CPU\n","\n","    return predictions, idxs"],"metadata":{"id":"O4NdWMTlMmCj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_predictions = {}\n","\n","# Generate predictions for each model\n","for name, model in models.items():\n","    print(f\"Generating predictions with {name}...\")\n","    preds, _ = test(model, test_loader, device)  # Use test function to get predictions\n","    model_predictions[name] = preds  # Store predictions\n"],"metadata":{"id":"QAxmBfDgfDtd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def weighted_voting(model_predictions, weights):\n","    num_classes = len(next(iter(weights.values())))  # Number of classes (assumes consistent across models)\n","    final_predictions = []\n","\n","    # Iterate through each sample\n","    for i in range(len(next(iter(model_predictions.values())))):\n","        class_votes = np.zeros(num_classes)  # Initialize votes for all classes\n","\n","        # Accumulate weighted votes from each model\n","        for model_name in model_predictions:\n","            pred_class = model_predictions[model_name][i]\n","            class_votes[pred_class] += weights[model_name][pred_class]\n","\n","        # Select the class with the maximum weighted vote\n","        final_predictions.append(np.argmax(class_votes))\n","\n","    return final_predictions\n"],"metadata":{"id":"x4zpR13LfFX-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Perform weighted voting\n","ensemble_predictions = weighted_voting(model_predictions, model_weights)"],"metadata":{"id":"MlH2wDrYfHab"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","\n","# Save predictions to CSV\n","# df = pd.DataFrame({'idx': range(len(ensemble_predictions)), 'predicted_label': ensemble_predictions})\n","# csv_path = '/ensemble_local_test.csv'\n","# df.to_csv(csv_path, index=False)\n","# print(f\"Predictions saved to {csv_path}\")\n","\n","# Extract ground truth labels from the extra_test_dataset\n","# Extract ground truth labels from extra_test_dataset\n","true_labels = [label for _, label in extra_test_dataset]  # Access the second element (label) in each tuple\n","  # Corrected list comprehension\n","\n","# Evaluate ensemble predictions\n","from sklearn.metrics import classification_report\n","print(classification_report(true_labels, ensemble_predictions))\n"],"metadata":{"id":"0u8EgTxcfIqQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_predictions = {}\n","all_indices = None  # To store indices from one of the models (should be consistent across models)\n","\n","for name, model in models.items():\n","    print(f\"Generating predictions with {name}...\")\n","    preds, idxs = test(model, kaggle_test_loader, device)  # Get predictions and indices\n","    model_predictions[name] = preds  # Store predictions\n","\n","    # Save indices from the first model (all models should have consistent indices)\n","    if all_indices is None:\n","        all_indices = idxs\n","\n","ensemble_predictions = weighted_voting(model_predictions, model_weights)\n","\n","import pandas as pd\n","\n","# Save predictions to CSV\n","df = pd.DataFrame({'idx': all_indices, 'predicted_label': ensemble_predictions})\n","csv_path = 'ensemble_kaggle_test.csv'\n","df.to_csv(csv_path, index=False)\n","print(f\"Predictions saved to {csv_path}\")"],"metadata":{"id":"ylkqdQXyfKcc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"ohbARrGWMxkk"}},{"cell_type":"markdown","source":["Hyperparameter Tunning\n"],"metadata":{"id":"uQ0RVtX_FBPE"}},{"cell_type":"code","source":["import torch\n","import random\n","import numpy as np\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms, models\n","from datasets import load_dataset\n","from tqdm import tqdm\n","import pandas as pd\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","# Set a fixed random seed for reproducibility\n","SEED = 42\n","random.seed(SEED)\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","if torch.cuda.is_available():\n","    torch.cuda.manual_seed(SEED)\n","    torch.cuda.manual_seed_all(SEED)\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False\n","\n","\n","# Define the Custom Dataset Class\n","class CustomHFDataset(Dataset):\n","    def __init__(self, hf_dataset, transform=None, test_flag=False):\n","        \"\"\"\n","        Args:\n","            hf_dataset: A split of the Hugging Face dataset (e.g., train, val, test)\n","            transform: PyTorch transforms to apply on the images\n","            test_flag: If True, returns indices instead of labels (for testing).\n","        \"\"\"\n","        self.hf_dataset = hf_dataset\n","        self.transform = transform\n","        self.test_flag = test_flag\n","\n","    def __len__(self):\n","        return len(self.hf_dataset)\n","\n","    def __getitem__(self, idx):\n","        example = self.hf_dataset[idx]\n","        image = example['image']\n","        label = example.get('label', -1)  # Default to -1 if no label exists (e.g., test set)\n","        index = example.get('idx', idx)  # Default to index if 'idx' is not explicitly provided\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        if self.test_flag:\n","            return image, index  # Return image and index for testing\n","        else:\n","            return image, label  # Return image and label for training\n","\n","\n","# Define Focal Loss Class\n","class FocalLoss(nn.Module):\n","    def __init__(self, alpha=1.0, gamma=2.0, reduction='mean'):\n","        super(FocalLoss, self).__init__()\n","        self.alpha = alpha\n","        self.gamma = gamma\n","        self.reduction = reduction\n","\n","    def forward(self, inputs, targets):\n","        logpt = F.log_softmax(inputs, dim=1)\n","        pt = torch.exp(logpt)\n","        logpt = logpt.gather(1, targets.unsqueeze(1)).squeeze(1)\n","        pt = pt.gather(1, targets.unsqueeze(1)).squeeze(1)\n","\n","        focal_loss = -self.alpha * (1 - pt) ** self.gamma * logpt\n","\n","        if self.reduction == 'mean':\n","            return focal_loss.mean()\n","        elif self.reduction == 'sum':\n","            return focal_loss.sum()\n","        else:\n","            return focal_loss\n","\n","\n","# Define Model Trainer Class\n","class ModelTrainer:\n","    def __init__(self, model, criterion, optimizer, device):\n","        \"\"\"\n","        Args:\n","            model: PyTorch model to train.\n","            criterion: Loss function.\n","            optimizer: Optimizer for training.\n","            device: Device to run training on (CPU/GPU).\n","        \"\"\"\n","        self.model = model.to(device)\n","        self.criterion = criterion\n","        self.optimizer = optimizer\n","        self.device = device\n","\n","    def train(self, train_loader, num_epochs=12):\n","        \"\"\"\n","        Train the model for a specified number of epochs.\n","\n","        Args:\n","            train_loader: DataLoader for training data.\n","            num_epochs: Number of epochs to train.\n","\n","        Returns:\n","            Trained model.\n","        \"\"\"\n","        for epoch in range(num_epochs):\n","            self.model.train()\n","            running_loss = 0.0\n","\n","            for images, labels in tqdm(train_loader):\n","                images, labels = images.to(self.device), labels.to(self.device)\n","\n","                # Zero gradients\n","                self.optimizer.zero_grad()\n","\n","                # Forward pass and compute loss\n","                outputs = self.model(images)\n","                loss = self.criterion(outputs, labels)\n","\n","                # Backward pass and optimization step\n","                loss.backward()\n","                self.optimizer.step()\n","\n","                running_loss += loss.item()\n","\n","            print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {running_loss / len(train_loader):.4f}\")\n","\n","    def evaluate(self, test_loader):\n","        \"\"\"\n","        Evaluate the model on a test set.\n","\n","        Args:\n","            test_loader: DataLoader for test data.\n","\n","        Returns:\n","            predictions: List of predicted labels.\n","            true_labels: List of ground truth labels.\n","        \"\"\"\n","        self.model.eval()\n","\n","        predictions = []\n","        true_labels = []\n","\n","        with torch.no_grad():\n","            for images, labels in test_loader:\n","                images, labels = images.to(self.device), labels.to(self.device)\n","\n","                outputs = self.model(images)\n","                _, predicted = outputs.max(1)\n","\n","                predictions.extend(predicted.cpu().numpy())\n","                true_labels.extend(labels.cpu().numpy())\n","\n","        return predictions, true_labels\n","\n","\n","# Define Utility Functions\n","def save_predictions_to_csv(predictions, dataset_test_split, output_path=\"test_predictions.csv\"):\n","    \"\"\"\n","    Save predictions along with indices to a CSV file.\n","\n","    Args:\n","      predictions: List of predicted labels.\n","      dataset_test_split: Test dataset containing indices.\n","      output_path: Path to save the CSV file.\n","    \"\"\"\n","    idx_column = [data['idx'] for data in dataset_test_split]\n","\n","    results_df = pd.DataFrame({\n","      'idx': idx_column,\n","      'predicted_label': predictions,\n","    })\n","\n","    results_df.to_csv(output_path, index=False)\n","    print(f\"Predictions saved to {output_path}!\")\n"],"metadata":{"id":"DZFo6mzGFDlk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Load Dataset and Define Transforms\n","dataset = load_dataset('hmdliu/ACAC-4K')\n","\n","train_transform = transforms.Compose([\n","transforms.ColorJitter(brightness=0.2, contrast=0.2),\n","transforms.ToTensor(),\n","transforms.Normalize(mean=[0.6127, 0.5398, 0.4442], std=[0.1288, 0.1147, 0.1009]),    ])\n","\n","test_transform = transforms.Compose([\n","transforms.ToTensor(),\n","transforms.Normalize(mean=[0.6127, 0.5398, 0.4442], std=[0.1288, 0.1147, 0.1009]),])\n","\n","\n","train_dataset = CustomHFDataset(dataset['train'], transform=train_transform)\n","test_dataset_split = dataset['test']\n","test_dataset = CustomHFDataset(test_dataset_split, transform=test_transform)\n","\n","train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=16)\n","\n","\n","num_classes = len(set(dataset['train']['label']))\n","\n","model_resnet18 = models.resnet18(weights=None)\n","model_resnet18.fc = nn.Linear(model_resnet18.fc.in_features, num_classes)\n","\n","device_used = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","criterion_focal_loss = FocalLoss(alpha=1.0)\n","\n","optimizer_adam_resnet18 = torch.optim.Adam(model_resnet18.parameters(), lr=5e-5)\n","\n","trainer_resnet18 = ModelTrainer(model=model_resnet18,\n","                                    criterion=criterion_focal_loss,\n","                                    optimizer=optimizer_adam_resnet18,\n","                                    device=device_used)\n","trainer_resnet18.train(train_loader=train_loader)\n","preds_resnet18, true_labels_resnet18 = trainer_resnet18.evaluate(test_loader=test_loader)\n","save_predictions_to_csv(predictions=preds_resnet18,\n","                            dataset_test_split=test_dataset_split,\n","                            output_path=\"test_predictions.csv\")\n","\n","model_name = \"resnet_hyper\"\n","model_path = f'{drive_model_dir}/{model_name}.pth'\n","torch.save(model.state_dict(), model_path)\n","print(f\"{model_name} saved to {model_path}\")\n"],"metadata":{"id":"nXU90Gi5FLLC"},"execution_count":null,"outputs":[]}]}